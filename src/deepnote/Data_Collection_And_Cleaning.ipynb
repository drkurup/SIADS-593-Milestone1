{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"deepnote_app_block_visible":true,"cell_id":"2a3ca7a0b82a4acb8b0752b1e33668be","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"cbea48e0c8024d399cabcacd10fb4b94"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"5c9fcecb81aa4f59b5cf8df5b8624ce6","deepnote_cell_type":"markdown"},"source":"# Data Collection & Cleaning of Ridership Project\n\n### Importing Necessary Libraries & Modules","block_group":"a78d8a9bd1da4cc0a219b131be164ebd"},{"cell_type":"code","metadata":{"source_hash":"54a43003","execution_start":1718328595561,"execution_millis":53,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"b9f5c0e4b9a44d49892e123a0895825e","deepnote_cell_type":"code"},"source":"import pandas as pd\nfrom opencage.geocoder import OpenCageGeocode","block_group":"35e627b40f964120bee2c0324e3f7a66","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"42cdf464f1024ab5a06cb9edcbf747d7","deepnote_cell_type":"markdown"},"source":"## Cleaning Overview\n\nFor this project we decided to use a number of datasets:\n\n1. Monthly Chicago Transit Authority (CTA) L (Metro System) Ridership data from the City of Chicago Data Portal\n2. 2000, 2010 & 2020 Population Data from the US Census Bureau for each Zip Code in Cook County, IL. Cook County includes includes Chicago and a number of suburbs in the Chicago Metro Area. \n3. CTA dataset containing descriptive information about various stations in the CTA L system\n\nOur goal was to combine all these datasets together, extract the relevant information, and analyze the information to get a better understanding of how population trends and ridership trends have changed over time and may be related with each other. This notebook includes the entire data collection, cleaning, and merging process that was developed as part of this project, which was consolidated into 9 functions. ","block_group":"533b2388af9b40448af4a950600fa346"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"0905561af32c41f29f2c3a8262e78fd0","deepnote_cell_type":"markdown"},"source":"## Primary Dataset Cleaning\n\nThe first dataset we explored contained CTA L Ridership Data. This was downloaded via CSV from the City of Chicago Data Portal. This dataset contains, for each L Stop, the number of entries into the station aggregated at a monthly level from January 1st, 2001 to February 28th, 2024. The dataset we downloaded has nearly 40,000 records, and by looking at the head below, we can see all of the columns available\n","block_group":"3954a5522c6d480790af46bf9886aadf"},{"cell_type":"markdown","metadata":{"source_hash":null,"execution_start":1718235948845,"execution_millis":68,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"0ce2e5175b774570a43e3ba98ae95b1e","deepnote_cell_type":"markdown"},"source":"`Convert to code block to see what the raw dataframe looks like`\n\n`dataset1 = pd.read_csv(\"siads_593_milestone1/src/data/CTARidershipMonthly_20240521.csv\")`\n`display(dataset1.head())`","block_group":"bedf90a63b9b411a88650ad368c77481"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"1a9967f42366443d96831914f0236f8c","deepnote_cell_type":"markdown"},"source":"We are looking to keep all the columns that contain ridership data, descriptive data, or data that can be used to merge this dataframe with the other datasets. To clean this dataset, we created the below function, `primary_dataset_cleaning`, in order to convert this CSV into a dataframe and keep relevant information. Eventually, we want to link this function to the City of Chicago Dataportal API, so we build this function in a way that can accomodate that with minimal changes. \n\nAll of the adjustments made to columns, and column drops were done to ensure they met the criteria outlined above.","block_group":"a670a037d7f44e8a929c98dba15eb6ef"},{"cell_type":"code","metadata":{"source_hash":"59d1a9da","execution_start":1718328599801,"execution_millis":14,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"baded30d64c146299d1bb6896b971a33","deepnote_cell_type":"code"},"source":"def primary_dataset_cleaning():\n    \n    # Converting CSV into dataframe\n    monthly_ridership_df = pd.read_csv(\"siads_593_milestone1/src/data/CTARidershipMonthly_20240521.csv\")\n    \n    # Change the column names to Upper Case to make it easier to clean\n    monthly_ridership_df_column = monthly_ridership_df.columns\n    monthly_ridership_df.columns = monthly_ridership_df_column.str.upper()\n    \n    # Rename STATIONAME to STATION_NAME to increase readability and ease of merging with other datasets\n    monthly_ridership_df.rename(columns = {'STATIONAME': 'STATION_NAME'}, inplace = True)\n\n    # Drop Station_ID column as it doesn't have relevant information for this project\n    monthly_ridership_df = monthly_ridership_df.drop(['STATION_ID'], axis = 1)\n\n    return monthly_ridership_df","block_group":"22517bb185784995b3fe6b37a6454de4","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"eb156de3","usedVariables":["monthly_ridership_df","pd","monthly_ridership_df_column"],"importedModules":[],"definedVariables":["primary_dataset_cleaning","monthly_ridership_df","monthly_ridership_df_column"]}},{"cell_type":"markdown","metadata":{"source_hash":null,"execution_start":1718235814127,"execution_millis":187,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"8ef5a1f3b46d4522a71171ffec2137ad","deepnote_cell_type":"markdown"},"source":"`Convert to code block to see what the Ridership dataframe looks like after cleaning`\n`display(primary_dataset_cleaning().head())`","block_group":"5f923e0a5dd643b39ec8df1ba646faf3"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"9d3442fc01134bdbb494a25080a9b5c9","deepnote_cell_type":"markdown"},"source":"Now that our primary dataset including ridership information has been cleaned, we want to clean our next dataset, which is Cook County population by Zip Code from the Decennial US Census. \n\nWhile all the data is from the same source, the US Census Bureau, the data was pulled from 3 different datasets. 2010 & 2020 data was from the same type of report, but for different Census', but that report wasn't available for 2000, therefore it had to be pulled from a different report that was formatted differently. The function below, `secondary_dataset_cleaning` takes all three datasets in CSV format, converts them into dataframes, and merges them together in a consistent format. ","block_group":"178501f3b02246c0b68f3e5037ab732a"},{"cell_type":"markdown","metadata":{"source_hash":"fe4ab45f","execution_start":1718242711069,"execution_millis":192,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"5810fdfbe30844428acbebba36901f41","deepnote_cell_type":"markdown"},"source":"`Convert to code block to see what the raw dataframes look like`\n\n`dataset2_2000 = pd.read_csv(\"siads_593_milestone1/src/data/DECENNIALSF12000.H002-2024-05-27T193225.csv\")`\n`dataset2_2010 = pd.read_csv(\"siads_593_milestone1/src/data/DECENNIALSF12010.P1-Data.csv\")`\n`dataset2_2020 = pd.read_csv(\"siads_593_milestone1/src/data/DECENNIALDHC2020.P1-Data.csv\")`\n\n`display(dataset2_2000.head())`\n`display(dataset2_2010.head())`\n`display(dataset2_2020.head())`","block_group":"71ef3bcaa956408faa3322ab0effc66f"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"c9d073ce7766482e9694fb8787598619","deepnote_cell_type":"markdown"},"source":"## Secondary Dataset Cleaning","block_group":"60b2ae978a9142db95294ad75c6b3705"},{"cell_type":"code","metadata":{"source_hash":"315d6fd4","execution_start":1718328603954,"execution_millis":336,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"eaf857eb4f734821a5cb515fbc0d2ffa","deepnote_cell_type":"code"},"source":"def secondary_dataset_cleaning():\n    \n    # Create Dataframes for each years Census Data\n    pop_2000 = pd.read_csv(\"siads_593_milestone1/src/data/DECENNIALSF12000.H002-2024-05-27T193225.csv\")\n    pop_2010 = pd.read_csv(\"siads_593_milestone1/src/data/DECENNIALSF12010.P1-Data.csv\")\n    pop_2020 = pd.read_csv(\"siads_593_milestone1/src/data/DECENNIALDHC2020.P1-Data.csv\")\n\n    # Begin cleaning of 2000 DataFrame   \n    # 2000 Dataset does not match 2010 & 2020 Datsets. Manipulating pop_2000 to match 2010/2020 format\n\n    # Index 0 has secondary headers that are not relevant for the purpose of this project\n    pop_2000_cleaning = pop_2000.drop(pop_2000.index[1:], axis = 0)\n    \n    # Transposing to change zip codes from column headers to row headers\n    pop_2000_cleaning = pop_2000_cleaning.transpose()\n\n    # We want the Zip Codes to be in a column itself, rather than as the dataframes index\n    pop_2000_cleaned = pop_2000_cleaning.reset_index().drop(0, axis = 0)\n    \n    # Renaming columns to make it easier to merge with the other dataframes\n    pop_2000_cleaned.columns = ['NAME', 'P2000']\n\n    # Removing commas from population numbers since 2010/2020 data does not have them\n    pop_2000_cleaned['P2000'] = pop_2000_cleaned['P2000'].str.replace(',','')\n\n    # End cleaning of 2000 Dataframe\n\n\n    # Begin cleaning of 2010 & 2020 DataFrames\n\n    #Removing unneeded columns\n    pop_2010_cleaned = pop_2010.drop(0, axis = 0).drop(columns = ['GEO_ID', 'Unnamed: 3'])\n    pop_2020_cleaned = pop_2020.drop(0, axis = 0).drop(columns = ['GEO_ID', 'Unnamed: 3'])\n\n    # End cleaning of 2010 & 2020 DataFrames\n\n    # Merging all three dataframes together and renaming columns to be more usable\n    total_pop = (pop_2000_cleaned.merge(pop_2010_cleaned, on = \"NAME\", how = 'outer')\n        .merge(pop_2020_cleaned, on = 'NAME', how = 'outer')\n    )\n    total_pop.columns = ['ZIP', '2000 Population', '2010 Population', '2020 Population']\n\n    # Removing \"ZCTA 5\" to keep just the Zip Code\n    total_pop['ZIP'] = total_pop['ZIP'].str.replace('ZCTA5 ', '')\n\n    columns = ['2000 Population', '2010 Population', '2020 Population']\n\n    total_pop[columns] = total_pop[columns].apply(lambda col: pd.to_numeric(col, errors = 'coerce'))\n\n    total_pop = total_pop.dropna(subset = ['2000 Population', '2010 Population'], how = 'all')\n    \n    # Call remove_nan function to remove nan values from total_pop\n    total_pop_without_nan = remove_nan(total_pop)\n\n    return total_pop_without_nan","block_group":"10f1b9533d5542a29272abfb7dcfffc7","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"4609b501","usedVariables":["total_pop","pop_2000","pop_2020","pop_2000_cleaned","pop_2020_cleaned","pd","pop_2010","pop_2010_cleaned","pop_2000_cleaning"],"importedModules":[],"definedVariables":["total_pop","pop_2000","pop_2020","pop_2000_cleaned","pop_2020_cleaned","secondary_dataset_cleaning","pop_2010","pop_2010_cleaned","pop_2000_cleaning"]}},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"f768ecaef5a4419fad4977c66e88d81d","deepnote_cell_type":"markdown"},"source":"`Convert to code block to see what the population dataframe looks like after cleaning`\n`display(secondary_dataset_cleaning().head())`","block_group":"f9e954d4ba5d45caa25d9b96cdb0c9a3"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"80cb5b463f9e45b9a7b49ce32d07dda6","deepnote_cell_type":"markdown"},"source":"### Removing NaN Values from Secondary Dataset\n\nAfter cleaning our population datasets, we noticed there are a number of `NaN` values. Some of them are due to the structure of the 2000 Dataset. To delinate between Zip Code Groups, the Census Bureau added dummy values. For example, when the Zip Codes starting with 605 (e.g. 60545) end, they add a dummy value of 605HH before starting the ZIP codes that start with 606. The rows with these dummy ZIP codes had NaN values for all three rows. Those rows were just dropped within secondary_dataset_cleaning itself. \n\nFor the rest of the `NaN` values, the function `remove_nan` was created, which is called within the `secondary_dataset_cleaning` function. For most of the other rows that contained `NaN` values, data wasn't available for either 2000 or 2020. Using the change between two consecutive Census', we extrapolated the population to replace `NaN` values using absoluate nominal change. If the 2000 value for a row was `NaN`, we reversed the change from 2010 to 2020. If the 2020 value was missing, we carried forward the change from 2000 to 2010 to replace the `NaN` values. There were a few rows that had `NaN` values in two of the columns. When those ZIP Codes were observed on a map, we saw that those ZIP codes were far from the City of Chicago and any L stops that were outside of Chicago. Therefore, those rows were also dropped within `secondary_dataset_cleaning itself`. ","block_group":"0f247783e46a45fbade67c6b47053554"},{"cell_type":"code","metadata":{"source_hash":"115bd85e","execution_start":1718328616707,"execution_millis":476,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"f2888e4786fc4008b5f9a625745482fe","deepnote_cell_type":"code"},"source":"def remove_nan(total_pop):\n\n    rows_with_nan = total_pop[total_pop.isna().any(axis = 1)]\n\n    for index, row in rows_with_nan.iterrows():\n        \n        # If NaN value is for 2000 population\n        if pd.isna(row['2000 Population']):\n            row_population_2010 = row['2010 Population']\n            row_population_2020 = row['2020 Population']\n\n            # Used nominal change instead of percentage change\n            nominal_change = row_population_2020 - row_population_2010\n\n            # Calulate and update dataframe with extrapoplated value\n            # Extrapolation uses subtraction because we are moving backwards in time\n            extrapolate_row_population_2000 = int(row_population_2010 - nominal_change)\n            \n            if extrapolate_row_population_2000 < 1:\n                total_pop.at[index, '2000 Population'] = 0\n            \n            else:\n                total_pop.at[index, '2000 Population'] = extrapolate_row_population_2000\n\n        # If NaN value is for 2020 population\n        if pd.isna(row['2020 Population']):\n            row_population_2000 = row['2000 Population']\n            row_population_2010 = row['2010 Population']\n\n            nominal_change = row_population_2010 - row_population_2000\n\n            # Calulate and update dataframe with extrapoplated value\n            # Extrapolation uses addition because we are moving forward in time\n            extrapolate_row_population_2020 = int(row_population_2010 + nominal_change)\n            \n            if extrapolate_row_population_2020 < 1:\n                total_pop.at[index, '2020 Population'] = 0\n            \n            else:\n                total_pop.at[index, '2020 Population'] = extrapolate_row_population_2020\n    \n    return total_pop","block_group":"4f2dae843f5241d69d71e0c85075413a","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"2aed8cb25fee41b1819ed240c10ba0e0","deepnote_cell_type":"markdown"},"source":"## Tertiary Dataset Cleaning\n\n### Zip Code Identification and Collection using longitude and latitude\n\nThe third dataset we used contains information about each CTA L Stop. We want to use that information to be able to compare our ridership data with our population by ZIP Code data. The third dataset does not have the ZIP Code of each station, but it does have the longitude and latitude. Using the OpenCage's Geocoding API we were able to get the Zip Code for each L Stop by passing the API our latitude and longitude. The below functions format the data we have in a way that the Geocading API can read and retrieve the ZIP Code. ","block_group":"25074feee07d48679de1b2f8439c636a"},{"cell_type":"code","metadata":{"source_hash":"fcc2a9e9","execution_start":1718328622725,"execution_millis":161,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"c93a0f0da4044f09b4d3e0d7ff720416","deepnote_cell_type":"code"},"source":"# Zip Code Functions\n\ndef extract_lat_lon(location):\n    location = location.strip('()')\n    latitude, longitude = location.split(',')\n    return float(latitude), float(longitude)\n\n# Register following this url and get the API Key\n# https://opencagedata.com/dashboard#geocoding\n\n\ndef get_zip_code(latitude, longitude):\n    # read key and create geocoder\n    # We originally have the key in Environment variable, but hard cording here for project report submission\n\n    key = \"606e648813dc4bbabeb0a392445c621f\"\n    geocoder = OpenCageGeocode(key)\n    \n    result = geocoder.reverse_geocode(latitude, longitude)\n    if result and len(result):\n        for postcode in result[0]['components']:\n            return result[0]['components']['postcode']\n\ndef apply_get_zip_code(row):  \n    return get_zip_code(row['latitude'], row['longitude'])","block_group":"c5f7df66430149ea881f8e88396079bb","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"b623e53d","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"71418c0a844649bfafb7627aa754618b","deepnote_cell_type":"markdown"},"source":"tertiary_dataset_cleaning takes dataset 3, removes all the columns we don't need, and then pulls the ZIP Code using the ZIP Code functions above. ","block_group":"a46b4a411e0a4224b2b8472ed1968d4c"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"3cf92bec29bb4439a8ad4fab8cea410a","deepnote_cell_type":"markdown"},"source":"`Convert to code block to see what the raw dataframe looks like`\n\n`dataset3 = pd.read_csv(\"siads_593_milestone1/src/data/CTA_Stops_20240526.csv\")`\n`display(dataset3.head())`","block_group":"aa96889e56cb4549ab834bfe4dc955fc"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"4150360c91574f09865b658bb38a2b58","deepnote_cell_type":"markdown"},"source":"### Tertiary Cleaning function including Zip Code functions defined above\n\nThe free version of OpenCage's Geocoding API has a rate limit of 2,500 pulls daily. Considering we have nearly 180 L Stops, we downloaded a CSV of our finished product in case the rate limit was breached (which happened many times). load_data() attempts to call the API, but if we breach the ratelimit will use our CSV as a backup. ","block_group":"d709925f43144cf6b396023ea2957da8"},{"cell_type":"code","metadata":{"source_hash":"5e192a7f","execution_start":1718328628349,"execution_millis":41,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"bf3f991831dd431b91d704d7fe13b85e","deepnote_cell_type":"code"},"source":"def tertiary_dataset_cleaning():\n\n    try:\n    \n        cta_l_stop_df = pd.read_csv(\"siads_593_milestone1/src/data/CTA_Stops_20240526.csv\")\n\n        # Get rid of all columns we don't want\n        cta_l_stop_df = (cta_l_stop_df[['DIRECTION_ID', 'STOP_NAME', 'STATION_NAME',\n        'STATION_DESCRIPTIVE_NAME', 'Location']])\n\n        # Many stations allow individuals to travel in multiple directions. We only care about the station itself\n        # therefore we are dropping Direction_ID and removing all the duplicate Station_Descriptive_Names\n        \n        cta_l_stop_df = cta_l_stop_df.drop_duplicates(subset=['STATION_DESCRIPTIVE_NAME'])\n        cta_l_stop_df = cta_l_stop_df.drop(['DIRECTION_ID', 'STOP_NAME'], axis = 1)\n\n        cta_l_stop_df['STATION_DESCRIPTIVE_NAME'] = (cta_l_stop_df['STATION_DESCRIPTIVE_NAME']\n        .replace(r'\\s+', '', regex=True)\n        )\n\n        cta_l_stop_df[['latitude', 'longitude']] = (cta_l_stop_df['Location']\n        .apply(lambda x: pd.Series(extract_lat_lon(x)))\n        )\n\n        cta_l_stop_df['zip_code'] = cta_l_stop_df.apply(apply_get_zip_code, axis=1)\n\n        cta_l_stop_df = cta_l_stop_df[['STATION_NAME', 'STATION_DESCRIPTIVE_NAME', 'zip_code', 'latitude', 'longitude']]\n\n    except Exception as e:\n\n        cta_l_stop_df = pd.read_csv('siads_593_milestone1/src/data/tertiary_dataset_cleaning_output_061224.csv')    \n\n    return cta_l_stop_df","block_group":"f01a7c24432e40f4a21e7f8bfbd1f28b","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"7d3845e2","usedVariables":["pd","cta_l_stop_df"],"importedModules":[],"definedVariables":["cta_l_stop_df","tertiary_dataset_cleaning"]}},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"89a83d654197448982511b1100a08db1","deepnote_cell_type":"markdown"},"source":"`Convert to code block to see what the population dataframe looks like after cleaning`\n`display(tertiaryy_dataset_cleaning().head())`","block_group":"88020c601f9947b385e1da74b5046165"},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"a52024bc8e494a438dfa08de837383a8","deepnote_cell_type":"markdown"},"source":"## Merging cleaned DataFrames together\n\nNow that all our datasets have been cleaned, we created the functions merge_secondary_and_tertiary and final_merge to merge all three of our dataframes together","block_group":"28853bea845044bbb3d13cdfeb0280bc"},{"cell_type":"code","metadata":{"allow_embed":false,"source_hash":"edcc9c3c","execution_start":1718328633032,"execution_millis":50,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"991fd567935b451c935b8267d6e45349","deepnote_cell_type":"code"},"source":"def merge_secondary_and_tertiary():\n    secondary = secondary_dataset_cleaning()\n    #If teritiary dataset fails on opencage api, provide the backup tertiary dataset cleaned csv \n    tertiary = tertiary_dataset_cleaning()\n\n    population_station_df = pd.merge(tertiary, secondary, left_on = 'zip_code', right_on = 'ZIP')\n\n    population_station_df = population_station_df[['zip_code', 'STATION_DESCRIPTIVE_NAME',\n    '2000 Population', '2010 Population', '2020 Population', 'latitude', 'longitude']]\n\n    return population_station_df","block_group":"58182d85dc1942348c2291b8d2e85454","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"allow_embed":false,"source_hash":"6ec52790","execution_start":1718328637906,"execution_millis":10,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"589c574ce0994bbaa8b40e7763b0ebe8","deepnote_cell_type":"code"},"source":"# Attempt to fix this code\n\ndef final_merge():\n\n    primary = primary_dataset_cleaning()\n    secondary_and_tertiary = merge_secondary_and_tertiary()\n\n    # 142 unique names in ridership. Created a lookup csv with STATION_NAME mapping\n    # import the mapping table\n    station_name_mapping = pd.read_csv(\"siads_593_milestone1/src/data/station_lookup_table.csv\")\n\n    station_name_mapping_selected = station_name_mapping[['RIDERSHIP_STATION_NAME', 'STATION_NAME']]\n\n    # First merge between primary dataset, and lookup table. Named merged_ridership_station_df\n    merged_ridership_station_df = (pd.merge(primary, station_name_mapping_selected,\n    left_on='STATION_NAME', right_on='RIDERSHIP_STATION_NAME')\n    )\n\n    # Second merge between merged_ridership_station_df and tertiary dataset\n    merged_ridership_cta_l_stop_df = (pd.merge(merged_ridership_station_df, secondary_and_tertiary,\n    left_on='STATION_NAME_y', right_on='STATION_DESCRIPTIVE_NAME')\n    )\n    \n    merged_ridership_cta_l_stop_df = merged_ridership_cta_l_stop_df[\n        ['STATION_DESCRIPTIVE_NAME', 'MONTH_BEGINNING', \n    'AVG_WEEKDAY_RIDES', 'AVG_SATURDAY_RIDES', 'AVG_SUNDAY-HOLIDAY_RIDES', 'MONTHTOTAL',\n    'zip_code', '2000 Population', '2010 Population', '2020 Population', 'latitude', 'longitude']\n    ]\n\n    return merged_ridership_cta_l_stop_df","block_group":"d3641ea377a14874939553d3db9c9b8f","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"deepnote_app_block_visible":true,"cell_id":"d1f8e21f40754f31bc44879f70a8084b","deepnote_cell_type":"markdown"},"source":"## Final Output\n\nThe below function was created to create a singular dataframe that can then be imported as part of the overall ipynb file for other notebooks. Try & Except were used to prevent our functions from breaking in the case of any dependency issues that arise post submission","block_group":"7394d98ff41c4f28b8e9affdc29754dd"},{"cell_type":"code","metadata":{"source_hash":"cf68d433","execution_start":1718328641122,"execution_millis":56,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"c67dd1d707e047dbab44d5f7421ea5b8","deepnote_cell_type":"code"},"source":"def load_finalmerge_data():\n    try:\n        # Try to call the API to get the data\n        df = final_merge()\n    except Exception as e:\n        # If API Rate Limit is exceeded, load from CSV\n        df = pd.read_csv('siads_593_milestone1/src/data/final_merge_061124.csv')\n    return df","block_group":"61e6c7ad4b9742a5b05f519d559235a3","execution_count":9,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"da4c386","execution_start":1718328646411,"execution_millis":67425,"deepnote_to_be_reexecuted":false,"deepnote_app_block_visible":true,"cell_id":"28f8555938e843b295666a655d4c97d8","deepnote_cell_type":"code"},"source":"final_merge_df = load_finalmerge_data()","block_group":"fa1546d0267043fd8fa437f784a181d9","execution_count":10,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7862ac62-05ae-412f-9bc1-59d586e87203' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_app_layout":"powerful-article","deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"93d1e475c11444ae8e1a673277671549","deepnote_execution_queue":[]}}